name: Intent Model Trainer
description: Component for training intent classification model using BERT
inputs:
  - {name: train_processed, type: Dataset}          # Processed training data from preprocessor
  - {name: val_processed, type: Dataset}            # Processed validation data from preprocessor
  - {name: preprocessing_metadata, type: String}     # Metadata from preprocessor
  - {name: intent_mapping, type: String}            # Intent mapping from preprocessor
  - {name: model_name, type: String, default: "bert-base-uncased"}
  - {name: batch_size, type: Integer, default: 32}
  - {name: epochs, type: Integer, default: 10}
  - {name: learning_rate, type: Float, default: 0.00002}
  - {name: dropout, type: Float, default: 0.5}
  - {name: hidden_size, type: Integer, default: 768}
outputs:
  - {name: model, type: Model}                      # Trained model in ONNX format (.pb)
  - {name: training_metadata, type: String}         # Training configuration and results
  - {name: training_logs, type: String}             # Detailed training logs
implementation:
  container:
    image: ravidocker189/final_model:4
    command: ["python"]
    args: [
      -u,model_trainer.py,
      "--train-data", {inputPath: train_processed},
      "--val-data", {inputPath: val_processed},
      "--preprocessing-metadata", {inputPath: preprocessing_metadata},
      "--intent-mapping", {inputPath: intent_mapping},
      "--model-name", {inputValue: model_name},
      "--batch-size", {inputValue: batch_size},
      "--epochs", {inputValue: epochs},
      "--learning-rate", {inputValue: learning_rate},
      "--dropout", {inputValue: dropout},
      "--hidden-size", {inputValue: hidden_size},
      "--output-paths",
      {outputPath: model},
      {outputPath: training_metadata},
      {outputPath: training_logs}
    ]
    env:
      PYTHONUNBUFFERED: "1"
      PYTHONDONTWRITEBYTECODE: "1"
      TRANSFORMERS_CACHE: "/tmp/transformers_cache"
      HF_HOME: "/tmp/transformers_cache"
      HF_HUB_CACHE: "/tmp/transformers_cache"
      HUGGINGFACE_HUB_CACHE: "/tmp/transformers_cache"
      HF_HUB_DOWNLOAD_TIMEOUT: "1000"
      TRANSFORMERS_OFFLINE: "0"
      HF_DATASETS_OFFLINE: "0"
      HF_HUB_OFFLINE: "0"
